# -*- coding: utf-8 -*-
"""preprocessor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HvTb4ZmzQgXw69x-n3ss81EuMl1vgXJH
"""

from typing import OrderedDict
import collections
from nltk.stem import PorterStemmer
import re
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')


class Preprocessor:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.ps = PorterStemmer()
        self.tfdict = OrderedDict({})

    def get_doc_id(self, doc):
        """ Splits each line of the document, into doc_id & text.
            Already implemented"""
        arr = doc.split("\t")
        # print(arr)
        return int(arr[0]), arr[1]

    def tokenizer(self, text, id):
        """ Implement logic to pre-process & tokenize document text.
            Write the code in such a way that it can be re-used for processing the user's query.
            To be implemented."""

        # id,content = self.get_doc_id(text)
        content = text.lower().strip()
        content = re.sub(r'\W+', ' ', content)
        content = content.split()
        content = [i for i in content if i not in self.stop_words]
        content = [self.ps.stem(word) for word in content]
        for i in content:
          if i not in self.tfdict:
            self.tfdict[i] = {}
          self.tfdict[i][id] = content.count(i)/len(content)
          self.tfdict[i] = OrderedDict(sorted(self.tfdict[i].items()))

        return content, self.tfdict
        # raise NotImplementedError

    def tokenize_query(self, query):
      
      content = query.lower().strip()
      content = re.sub(r'\W+', ' ', content)
      content = content.split()
      content = [i for i in content if i not in self.stop_words]
      content = [self.ps.stem(word) for word in content]
      print(content)
      return content

pp = Preprocessor()
pp.tokenize_query("I am a boy")

